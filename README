create a virtualenv, activate it, and install requirements in it:

pip install -r requirements.txt

overview
========

search.py 
=========

does the search, pulling fetching sets of 4 results, and 
working around google's throttling.  
Output goes into data.txt

TODO: Replace with a real screen scraper, since the API limits to
64 results


select.py 
=========

parses the repo names out of data.txt and fetches the data on each
repo from github.
Result goes into a dict, which is pickled into pickled.pic


show.py
=======

Simply shows what's in the pickle file produced by select.py


gather.py
=========

gathers items into a tree and does some rudimentary scoring.
Takes input from the pickle file, at the moment.

TODO
====
get the 8.5k django search data.
put results to google docs

